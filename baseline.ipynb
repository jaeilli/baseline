{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv('C:/Users/Jaeil/Desktop/Jaeil/PSAT/airbnb_train.csv',encoding='CP949')\n",
    "train.info()\n",
    "\n",
    "#look for object type features\n",
    "train.describe(include=['O'])\n",
    "\n",
    "c = data.select_dtypes(np.object).columns\n",
    "data[c] = data[c].astype('category')\n",
    "\n",
    "# one hot encode\n",
    "my_cols = ['district', 'refund_rule', 'type_airbnb']\n",
    "df_onehot = pd.get_dummies(data[my_cols])\n",
    "print(\"# of columns after one-hot encoding: {0}\".format(len(df_onehot.columns)))\n",
    "\n",
    "#drop col that are one-hot enconded\n",
    "data = data.drop(my_cols, axis=1)\n",
    "\n",
    "#cbind with one-hot encoded df\n",
    "data_onehot = pd.concat([data, df_onehot], axis=1)\n",
    "\n",
    "#predict\n",
    "X_train = train.drop(\"Survived\", axis=1)\n",
    "y_train = train[\"Survived\"]\n",
    "X_test  = test.drop(\"PassengerId\", axis=1).copy()\n",
    "X_train.shape, y_train.shape, X_test.shape\n",
    "\n",
    "X = data_final.drop('room_price', axis=1)\n",
    "y = data_final['room_price']\n",
    "\n",
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 1234)\n",
    "\n",
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_val.shape)\n",
    "print('Testing Labels Shape:', y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb\n",
    "import xgboost as xgb\n",
    "\n",
    "#dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=500 )\n",
    "\n",
    "xgb_params = {\n",
    "'min_child_weight': [0.1, 1, 5, 10],\n",
    "'gamma': [0.5, 1,  2, 5],\n",
    "'subsample': [0.6, 0.8, 1.0],\n",
    "'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "'max_depth': [4,6,8,10],\n",
    "'learning_rate': [0.01, 0.1, 1],\n",
    "#'n_estimators': [500]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,random_state=1234,shuffle=True)\n",
    "\n",
    "xgb_model = RandomizedSearchCV(estimator = xgb_clf,\n",
    "                                param_distributions = xgb_params,\n",
    "                                scoring='roc_auc', \n",
    "                                cv = skf , \n",
    "                                return_train_score = True).fit(X_train, y_train)\n",
    "\n",
    "# Fit to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "##### lgb\n",
    "import lightgbm as lgb\n",
    "lgb_clf = lgb.LGBMClassifier(num_boost_round=500)\n",
    "\n",
    "lgb_params = {\n",
    "'min_data_in_leaf': [1, 5, 10],\n",
    "'num_leaves': [2,5,10,20],\n",
    "'bagging_fraction': [0.6, 0.8, 1.0],\n",
    "'feature_fraction': [0.6, 0.8, 1.0],\n",
    "'max_depth': [4,6,8,10],\n",
    "'learning_rate': [0.01, 0.1, 1],\n",
    "'num_boost_round': [500],\n",
    "'save_binary':True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,random_state=1234,shuffle=True)\n",
    "\n",
    "lgb_model = RandomizedSearchCV(estimator = lgb_clf,\n",
    "                                param_distributions = lgb_params,\n",
    "                                scoring='roc_auc', \n",
    "                                cv = skf , \n",
    "                                return_train_score = True).fit(X_train, y_train)\n",
    "\n",
    "# Fit to the training data\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-0a434643291e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-0a434643291e>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    :mod:`sklearn.metrics`\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "LogisticRegressionCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "log_model = LogisticRegressionCV(penalty='l1', solver='liblinear', cv=skf,max_iter=1000,scoring = 'roc_auc',\n",
    "                                 random_state=123)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "print (log_model.score(X_test, y_test) ) # score is 0.775\n",
    "print (log_model.intercept_ ) #-1.83569557\n",
    "print (log_model.coef_)  # [ 0,  0, 0.65930981, 1.17808155] (sepal length, sepal width, petal length, petal width)\n",
    "print (log_model.C_ ) # optimal lambda: 0.35938137\n",
    "\n",
    "log_model_sel_dat = pd.DataFrame(np.transpose(np.vstack((X_train.columns, clf.coef_))))\n",
    "log_model_sel_dat.columns = ['colname' , 'coef']\n",
    "log_model_sel_dat.loc[log_model_sel_dat['coef'] == 0]\n",
    "\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "log_model = sm.Logit(y_train, X_train)\n",
    "\n",
    "log_result = log_model.fit()\n",
    "print(log_result.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "clf = LogisticRegressionCV(n_jobs=2, penalty='l1', solver='liblinear', cv=5, scoring = ‘accuracy’, random_state=0)\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "print clf.score(X_train_fit.transform(X_test), y_test)  # score is 0.775\n",
    "print clf.intercept_  #-1.83569557\n",
    "print clf.coef_  # [ 0,  0, 0.65930981, 1.17808155] (sepal length, sepal width, petal length, petal width)\n",
    "print clf.C_  # optimal lambda: 0.35938137\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "parameters = {\n",
    "    #\"n_estimators\": [500], \n",
    "    \"max_features\": [\"auto\", \"log2\"], \n",
    "    \"max_depth\": [1, 2, 3, 5, 10], \n",
    "    \"min_samples_split\": [2, 3, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,random_state=1234,shuffle=True)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = RandomizedSearchCV(RandomForestClassifier(n_jobs=4, n_estimators=500), parameters, cv=skf).fit(X_train, y_train)\n",
    "\n",
    "print(rf_model.best_score_)\n",
    "# print(accuracy_score(y_val, forest_model.predict(X_val)))\n",
    "print(rf_model.best_params_)\n",
    "print(rf_model.best_estimator_)\n",
    "print(\"Running time :\", time.time() - start, \"sec\")  # 실행 시간\n",
    "\n",
    "feat_importances = pd.Series(rf_model.best_estimator_.feature_importances_, index = X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh', title='RF Feature Importance').invert_yaxis()\n",
    "\n",
    "rf_params = rf_model.best_params_\n",
    "\n",
    "rf_best = RandomForestClassifier(**rf_params, random_state=10)\n",
    "rf_best.fit(X_train, y_train)\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "rf_train_prob = rf_best.predict_proba(X_train)[:,1]\n",
    "rf_pred_prob = rf_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "rf_filename = 'C:/Users/Jaeil/Desktop/Github/titanic/models/rf_1.sav' \n",
    "#rf_model.save(rf_filename)\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'finalized_model.sav'\n",
    "import joblib\n",
    "joblib.dump(rf_model, rf_filename)\n",
    "\n",
    "rf_loaded_model = joblib.load(rf_filename)\n",
    "result = rf_loaded_model.score(X_train, y_train)\n",
    "print(result)\n",
    "\n",
    "#joblib.dump(rf_model, 'rf_model_object.pkl')\n",
    "    \n",
    "############################################3\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_val.values, rf_best.predict(X_val))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "submission_rf = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": xgb_pred\n",
    "    })\n",
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "\n",
    "colnames = train.columns\n",
    "# Define dictionary to store our rankings\n",
    "ranks = {}\n",
    "# Create our function which stores the feature rankings to the ranks dictionary\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "ranks[\"RF\"] = ranking(rf.feature_importances_, colnames);\n",
    "\n",
    "# Create empty dictionary to store the mean value calculated from all the scores\n",
    "r = {}\n",
    "for name in colnames:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "    \n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print(\"\\t%s\" % \"\\t\".join(methods))\n",
    "for name in colnames:\n",
    "    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                         [ranks[method][name] for method in methods]))))\n",
    "    \n",
    "    \n",
    "# Put the mean scores into a Pandas dataframe\n",
    "meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n",
    "\n",
    "# Sort the dataframe\n",
    "meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n",
    "\n",
    "# Let's plot the ranking of the features\n",
    "sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n",
    "               size=14, aspect=1.9, palette='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking\n",
    "X_stack_train1 = np.concatenate((rf_train_prob1, xgb_train_prob1, lgb_train_prob1),axis=1)\n",
    "X_stack_test1 = np.concatenate((rf_pred_prob1, xgb_pred_prob1, lgb_pred_prob1),axis=1)\n",
    "\n",
    "stack_1 = LogisticRegression(random_state=123).fit(X_stack_train1, y_train)\n",
    "stack_pred_1 = stack_1.predict(X_stack_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_filename = 'C:/Users/Jaeil/Desktop/Github/titanic/models/rf_1.sav' \n",
    "#rf_model.save(rf_filename)\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'finalized_model.sav'\n",
    "import joblib\n",
    "joblib.dump(rf_model, rf_filename)\n",
    "########################################\n",
    "rf_loaded_model = joblib.load(rf_filename)\n",
    "rf_result = rf_loaded_model.score(X_train, y_train)\n",
    "print(rf_result)\n",
    "\n",
    "# check for the feature importance of each feature from the best model fitted by best parameter grid \n",
    "feat_importances = pd.Series(rf_model.best_estimator_.feature_importances_, index = X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh', title='RF Feature Importance').invert_yaxis()\n",
    "\n",
    "rf_params = rf_model.best_params_ \n",
    "\n",
    "rf_best = RandomForestClassifier(**rf_params, random_state=10) # set best parameter grid \n",
    "rf_best.fit(X_train, y_train) # fit best model to train data\n",
    "rf_pred = rf_best.predict(X_test) # predict y label by fitted model\n",
    "\n",
    "rf_train_prob = rf_best.predict_proba(X_train)[:,1]\n",
    "rf_pred_prob = rf_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 샘플링 된  data onehot encoding 전 데이터 따로 저장 후 train test split 한 데이터에 적용!!\n",
    "rf_train = pd.concat([X_init_train, pd.DataFrame(rf_best.predict(X_train), columns=['rf_pred']),\n",
    "                      pd.DataFrame(y_train)], axis=1)\n",
    "\n",
    "rf_test = pd.concat([X_init_test, pd.DataFrame(rf_best.predict(X_test), columns=['rf_pred']),\n",
    "                      pd.DataFrame(y_test)], axis=1)\n",
    "\n",
    "rf_train.to_csv('',index=False)\n",
    "rf_test.to_csv('',index=False)\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "def ks_stat(y, yhat):\n",
    "    return ks_2samp(yhat[y==1], yhat[y!=1]).statistic\n",
    "\n",
    "ks_stat(y_train, rf_best.predict(X_train))\n",
    "ks_stat(y_test, rf_best.predict(X_test))\n",
    "\n",
    "rf_train.groupby('Pclass').apply(lambda x: ks_stat(x['rf_pred'], x['Survived']))\n",
    "\n",
    "def calculate_psi(expected, actual, buckettype='bins', buckets=10, axis=0):\n",
    "    def psi(expected_array, actual_array, buckets):\n",
    "        def scale_range (input, min, max):\n",
    "            input += -(np.min(input))\n",
    "            input /= np.max(input) / (max - min)\n",
    "            input += min\n",
    "            return input\n",
    "\n",
    "\n",
    "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "\n",
    "        if buckettype == 'bins':\n",
    "            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "        elif buckettype == 'quantiles':\n",
    "            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    "\n",
    "\n",
    "\n",
    "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "\n",
    "        def sub_psi(e_perc, a_perc):\n",
    "            if a_perc == 0:\n",
    "                a_perc = 0.0001\n",
    "            if e_perc == 0:\n",
    "                e_perc = 0.0001\n",
    "\n",
    "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "            return(value)\n",
    "\n",
    "        psi_value = np.sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
    "\n",
    "        return(psi_value)\n",
    "\n",
    "    if len(expected.shape) == 1:\n",
    "        psi_values = np.empty(len(expected.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(expected.shape[axis])\n",
    "\n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = psi(expected, actual, buckets)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = psi(expected[:,i], actual[:,i], buckets)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = psi(expected[i,:], actual[i,:], buckets)\n",
    "\n",
    "    return(psi_values)\n",
    "\n",
    "calculate_psi(rf_best.predict(X_train), y_train, buckettype='quantiles', buckets=10, axis=1)\n",
    "rf_train.groupby('Pclass').apply(lambda x: calculate_psi(x['rf_pred'], x['Survived'],\n",
    "                                                        buckettype='quantiles', buckets=10, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "#xgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=500 )\n",
    "\n",
    "xgb_params = {\n",
    "'min_child_weight': [0.1, 1, 5, 10],\n",
    "'gamma': [0.5, 1,  2, 5],\n",
    "'subsample': [0.6, 0.8, 1.0],\n",
    "'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "'max_depth': [4,6,8,10],\n",
    "'learning_rate': [0.01, 0.1, 1],\n",
    "#'n_estimators': [500]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,random_state=123,shuffle=True)\n",
    "start = time.time()\n",
    "xgb_model = RandomizedSearchCV(estimator = xgb_clf,\n",
    "                                param_distributions = xgb_params,\n",
    "                                scoring='roc_auc', \n",
    "                                cv = skf , \n",
    "                                return_train_score = True).fit(X_train, y_train)\n",
    "\n",
    "# Fit to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_model.best_score_)\n",
    "# print(accuracy_score(y_val, forest_model.predict(X_val)))\n",
    "print(xgb_model.best_params_)\n",
    "print(xgb_model.best_estimator_)\n",
    "print(\"Running time :\", time.time() - start, \"sec\")  # 실행 시간\n",
    "\n",
    "xgb_filename = 'C:/Users/Jaeil/Desktop/Github/titanic/models/xgb_1.sav' \n",
    "#rf_model.save(rf_filename)\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'finalized_model.sav'\n",
    "import joblib\n",
    "joblib.dump(xgb_model, xgb_filename)\n",
    "########################################\n",
    "xgb_loaded_model = joblib.load(xgb_filename)\n",
    "xgb_params = lgb_loaded_model.best_params_ \n",
    "\n",
    "xgb_best = xgb.XGBClassifier(**xgb_params, n_estimators=500,random_state=10) # set best parameter grid \n",
    "xgb_best.fit(X_train, y_train) # fit best model to train data\n",
    "xgb_pred = xgb_best.predict(X_test) # predict y label by fitted model\n",
    "\n",
    "#train res\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train.values, xgb_best.predict(X_train))\n",
    "auc(fpr, tpr)\n",
    "\n",
    "#test res\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values, xgb_pred)\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# check for the feature importance of each feature from the best model fitted by best parameter grid \n",
    "feat_importances = pd.Series(xgb_model.best_estimator_.feature_importances_, index = X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh', title='RF Feature Importance').invert_yaxis()\n",
    "\n",
    "submission_xgb = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": xgb_pred\n",
    "    })\n",
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "\n",
    "##################################\n",
    "import lightgbm as lgb\n",
    "lgb_clf = lgb.LGBMClassifier(n_estimators=500,save_binary=True)\n",
    "\n",
    "lgb_params = {\n",
    "'min_data_in_leaf': [1, 5, 10],\n",
    "'num_leaves': [2,5,10,20],\n",
    "'bagging_fraction': [0.6, 0.8, 1.0],\n",
    "'feature_fraction': [0.6, 0.8, 1.0],\n",
    "'max_depth': [4,6,8,10],\n",
    "'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,random_state=1234,shuffle=True)\n",
    "start = time.time()\n",
    "lgb_model = RandomizedSearchCV(estimator = lgb_clf,\n",
    "                                param_distributions = lgb_params,\n",
    "                                scoring='roc_auc', \n",
    "                                cv = skf , \n",
    "                                return_train_score = True).fit(X_train, y_train)\n",
    "\n",
    "# Fit to the training data\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(lgb_model.best_score_)\n",
    "# print(accuracy_score(y_val, forest_model.predict(X_val)))\n",
    "print(lgb_model.best_params_)\n",
    "print(lgb_model.best_estimator_)\n",
    "print(\"Running time :\", time.time() - start, \"sec\")  # 실행 시간\n",
    "\n",
    "\n",
    "\n",
    "lgb_filename = 'C:/Users/Jaeil/Desktop/Github/titanic/models/lgb_1.sav' \n",
    "#rf_model.save(rf_filename)\n",
    "\n",
    "# save the model to disk\n",
    "#filename = 'finalized_model.sav'\n",
    "import joblib\n",
    "joblib.dump(lgb_model, lgb_filename)\n",
    "########################################\n",
    "lgb_loaded_model = joblib.load(lgb_filename)\n",
    "lgb_params = lgb_loaded_model.best_params_ \n",
    "\n",
    "lgb_best = lgb.LGBMClassifier(**lgb_params, n_estimators=500,random_state=10) # set best parameter grid \n",
    "lgb_best.fit(X_train, y_train) # fit best model to train data\n",
    "lgb_pred = lgb_best.predict(X_test) # predict y label by fitted model\n",
    "\n",
    "#train res\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_train.values, lgb_best.predict(X_train))\n",
    "auc(fpr, tpr)\n",
    "\n",
    "#test res\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values, lgb_pred)\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# check for the feature importance of each feature from the best model fitted by best parameter grid \n",
    "feat_importances = pd.Series(lgb_model.best_estimator_.feature_importances_, index = X_train.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh', title='RF Feature Importance').invert_yaxis()\n",
    "\n",
    "submission_lgb = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": lgb_pred\n",
    "    })\n",
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "\n",
    "\n",
    "################################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "rf_train_prob1 = rf_best.predict_proba(X_train)[:,1]\n",
    "rf_pred_prob1 = rf_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "xgb_train_prob1 = xgb_best.predict_proba(X_train)[:,1]\n",
    "xgb_pred_prob1 = xgb_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "lgb_train_prob1 = lgb_best.predict_proba(X_train)[:,1]\n",
    "lgb_pred_prob1 = lgb_best.predict_proba(X_test)[:,1]\n",
    "\n",
    "# stacking\n",
    "X_stack_train1 = pd.concat([pd.DataFrame(rf_train_prob1), pd.DataFrame(xgb_train_prob1), pd.DataFrame(lgb_train_prob1)], axis=1)\n",
    "X_stack_test1 = pd.concat([pd.DataFrame(rf_pred_prob1), pd.DataFrame(xgb_pred_prob1), pd.DataFrame(lgb_pred_prob1)], axis=1)\n",
    "\n",
    "stack_1 = LogisticRegression(random_state=123).fit(X_stack_train1, y_train)\n",
    "stack_pred_1 = stack_1.predict(X_stack_test1)\n",
    "\n",
    "# train res\n",
    "fpr, tpr, thresholds = roc_curve(y_train.values, stack_1.predict(X_stack_train1))\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# test res\n",
    "fpr, tpr, thresholds = roc_curve(y_test.values, stack_pred_1)\n",
    "auc(fpr, tpr)\n",
    "\n",
    "\n",
    "submission_stack1 = pd.DataFrame({\n",
    "        \"PassengerId\": test[\"PassengerId\"],\n",
    "        \"Survived\": stack_pred1\n",
    "    })\n",
    "submission.to_csv('../output/submission.csv', index=False)\n",
    "\n",
    "\n",
    "######################################################\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns    \n",
    "colnames = X_train.columns\n",
    "# Define dictionary to store our rankings\n",
    "ranks = {}\n",
    "# Create our function which stores the feature rankings to the ranks dictionary\n",
    "def ranking(ranks, names, order=1):\n",
    "    minmax = preprocessing.MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x,2), ranks)\n",
    "    return dict(zip(names, ranks))\n",
    "\n",
    "ranks[\"RF\"] = ranking(rf_model.best_estimator_.feature_importances_, colnames);\n",
    "ranks[\"XGB\"] = ranking(xgb_model.best_estimator_.feature_importances_, colnames);\n",
    "ranks[\"LGB\"] = ranking(lgb_model.best_estimator_.feature_importances_, colnames);\n",
    "\n",
    "# Create empty dictionary to store the mean value calculated from all the scores\n",
    "r = {}\n",
    "for name in colnames:\n",
    "    r[name] = round(np.mean([ranks[method][name] \n",
    "                             for method in ranks.keys()]), 2)\n",
    "    \n",
    "methods = sorted(ranks.keys())\n",
    "ranks[\"Mean\"] = r\n",
    "methods.append(\"Mean\")\n",
    " \n",
    "print(\"\\t%s\" % \"\\t\".join(methods))\n",
    "for name in colnames:\n",
    "    print(\"%s\\t%s\" % (name, \"\\t\".join(map(str, \n",
    "                         [ranks[method][name] for method in methods]))))\n",
    "    \n",
    "    \n",
    "# Put the mean scores into a Pandas dataframe\n",
    "meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n",
    "\n",
    "# Sort the dataframe\n",
    "meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n",
    "\n",
    "# Let's plot the ranking of the features\n",
    "sns.set(font_scale=3)\n",
    "sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", \n",
    "               height=14, aspect=3, palette='coolwarm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
